{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.array(x * (x > 0) != 0, dtype='int')\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 4/(np.exp(2*x) + 2 + np.exp(-2*x))\n",
    "\n",
    "_ACTIVATION_1 = tanh\n",
    "_ACTIVATION_1_DERIVATIVE = tanh_derivative\n",
    "\n",
    "_ACTIVATION_2 = tanh\n",
    "_ACTIVATION_2_DERIVATIVE = tanh_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int2binary = {}\n",
    "\n",
    "num_range = 2**8\n",
    "\n",
    "_DIGITS = 8\n",
    "\n",
    "binary = np.unpackbits(np.array([range(num_range)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(num_range):\n",
    "    int2binary[i] = binary[i]\n",
    "    \n",
    "def generate_row(num1, num2):\n",
    "    row = np.zeros((1, _DIGITS*3))\n",
    "    row[0,: _DIGITS*2:2] = int2binary[num1]\n",
    "    row[0,1:_DIGITS*2:2] = int2binary[num2]\n",
    "    row[0,_DIGITS*2:] = int2binary[num1+num2]\n",
    "    return row\n",
    "\n",
    "def data_generator(data_size):\n",
    "    data = np.zeros((data_size, _DIGITS*3))\n",
    "    for row_index in range(data_size):\n",
    "        a = np.random.randint(int(num_range/2))\n",
    "        b = np.random.randint(int(num_range/2))\n",
    "        data[row_index,:] = generate_row(a,b)[0,:]\n",
    "    return data\n",
    "\n",
    "def input_feed(data, batch_size):\n",
    "    for index in range(0, len(data), batch_size):\n",
    "        if index*batch_size + batch_size < len(data):\n",
    "            yield {'train_data': data[index:index + batch_size, :_DIGITS*2],\n",
    "                   'target_data': data[index:index + batch_size, _DIGITS*2:]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Rnn:\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        self.h_size = 10\n",
    "        self.o_size = 1\n",
    "        self.batch_size = None\n",
    "        self.lrate = None\n",
    "        \n",
    "        self.layers = {\n",
    "            'input': [],\n",
    "            'hidden': [],\n",
    "            'output': []\n",
    "        }\n",
    "        \n",
    "        self.synapse_ih = {\n",
    "            'weights': np.random.rand(input_dim, self.h_size),\n",
    "            'biases': np.random.rand(1, self.h_size)\n",
    "        } \n",
    "        self.synapse_hh = {\n",
    "            'weights': np.random.rand(self.h_size, self.h_size)\n",
    "        }\n",
    "        self.synapse_ho = {\n",
    "            'weights': np.random.rand(self.h_size, self.o_size),\n",
    "            'biases': np.random.rand(1, self.o_size)\n",
    "        }\n",
    "    \n",
    "    def train(self, train_data, batch_generator, batch_size=1, epochs=10, lrate=0.01):\n",
    "        self.batch_size = batch_size\n",
    "        self.lrate = lrate\n",
    "        data = copy.deepcopy(train_data)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_error = 0\n",
    "            data = sk.utils.shuffle(data)\n",
    "            for batch in batch_generator(data, self.batch_size):\n",
    "                prediction = np.array(self._forward_prop(batch['train_data'])).reshape(_DIGITS,self.batch_size).T\n",
    "                print(prediction)\n",
    "                error = (batch['target_data']-prediction)\n",
    "                self._back_prop(error)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(\"Epoch: {0} Train Error: {1}\".format(epoch, (error**2).sum()/len(train_data)))\n",
    "            \n",
    "    def test(self):\n",
    "        pass\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        temp_s = self.batch_size\n",
    "        self.batch_size = 1\n",
    "        prediction = self._forward_prop(x)\n",
    "        self.batch_size = temp_s\n",
    "        return prediction\n",
    "    \n",
    "    def _forward_prop(self, x):\n",
    "        \"\"\"assumes x is a sequence input and first dim is seq length\"\"\"\n",
    "        self.layers['input'] = []\n",
    "        self.layers['hidden'] = [np.zeros((self.batch_size, self.h_size))]\n",
    "        self.layers['output'] = []\n",
    "        # TODO: different iterator\n",
    "        for index in range(_DIGITS):\n",
    "            self.layers['input'].append(np.atleast_2d(x[:,index*2:index*2+2]))\n",
    "            self.layers['hidden'].append(_ACTIVATION_1(np.dot(self.layers['input'][-1], self.synapse_ih['weights']) + \n",
    "                                                      np.dot(self.layers['hidden'][-1], self.synapse_hh['weights']) +\n",
    "                                                      self.synapse_ih['biases']))\n",
    "            self.layers['output'].append(_ACTIVATION_2(np.dot(self.layers['hidden'][-1], self.synapse_ho['weights']) +\n",
    "                                                       self.synapse_ho['biases']))\n",
    "        \n",
    "        return self.layers['output']\n",
    "        \n",
    "    def _back_prop(self, errors):\n",
    "        synapse_update_ih = {\n",
    "            'weights': np.zeros_like(self.synapse_ih['weights']),\n",
    "            'biases': np.zeros_like(self.synapse_ih['biases'])\n",
    "        }\n",
    "        synapse_update_hh = {\n",
    "            'weights': np.zeros_like(self.synapse_hh['weights'])\n",
    "        }\n",
    "        synapse_update_ho = {\n",
    "            'weights': np.zeros_like(self.synapse_ho['weights']),\n",
    "            'biases': np.zeros_like(self.synapse_ho['biases'])\n",
    "        }\n",
    "        \n",
    "        delta_hh = np.zeros((1, self.h_size))\n",
    "        delta_next_hh = np.zeros((1, self.h_size))\n",
    "        delta_ho = [np.multiply(_ACTIVATION_2_DERIVATIVE(self.layers['output'][index]), errors[:,index].reshape(self.batch_size, self.o_size)) for index in range(_DIGITS)]\n",
    "        \n",
    "        for index in range(_DIGITS-1,-1,-1):\n",
    "            synapse_update_ho['weights'] +=  np.dot(self.layers['output'][index].T, delta_ho[index])\n",
    "            synapse_update_ho['biases'] +=  np.atleast_2d(delta_ho[index].sum(axis=0))\n",
    "            \n",
    "            delta_hh = np.multiply((np.dot(delta_next_hh, self.synapse_hh['weights'].T) + \n",
    "                         np.dot(delta_ho[index], self.synapse_ho['weights'].T)), _ACTIVATION_1_DERIVATIVE(self.layers['hidden'][index+1]))\n",
    "            \n",
    "            synapse_update_hh['weights'] += np.dot(self.layers['hidden'][index+1].T, np.atleast_2d(delta_hh))\n",
    "            \n",
    "            synapse_update_ih['weights'] += np.dot(self.layers['input'][index].T, np.atleast_2d(delta_hh))\n",
    "            synapse_update_ih['biases'] += np.atleast_2d(delta_hh.sum(axis=0))\n",
    "\n",
    "            delta_next_hh = delta_hh\n",
    "        \n",
    "        self.synapse_ih['weights'] +=  self.lrate * (synapse_update_ih['weights']/self.batch_size)\n",
    "        self.synapse_hh['weights'] +=  self.lrate * (synapse_update_hh['weights']/self.batch_size)\n",
    "        self.synapse_ho['weights'] +=  self.lrate * (synapse_update_ho['weights']/self.batch_size)\n",
    "        \n",
    "        self.synapse_ih['biases'] +=  self.lrate * (synapse_update_ih['biases']/self.batch_size)\n",
    "        self.synapse_ho['biases'] +=  self.lrate * (synapse_update_ho['biases']/self.batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9979721  0.9999743  0.99997453 0.99997453 0.99997453 0.99997453\n",
      "  0.99997453 0.99997453]\n",
      " [0.9979721  0.9999743  0.99997452 0.99997452 0.99997453 0.99997452\n",
      "  0.99997453 0.99997452]\n",
      " [0.9979721  0.99997342 0.99997452 0.99997449 0.99997452 0.99997453\n",
      "  0.99997453 0.99997453]\n",
      " [0.9979721  0.9999743  0.99997452 0.99997453 0.99997452 0.99997452\n",
      "  0.99997453 0.99997452]\n",
      " [0.9979721  0.99997342 0.99997453 0.99997453 0.99997452 0.99997452\n",
      "  0.99997452 0.99997449]\n",
      " [0.9979721  0.99997409 0.99997453 0.99997449 0.99997453 0.99997452\n",
      "  0.99997453 0.99997452]\n",
      " [0.9979721  0.99997342 0.99997453 0.99997452 0.99997452 0.99997449\n",
      "  0.99997452 0.99997453]\n",
      " [0.9979721  0.99997409 0.99997453 0.99997452 0.99997453 0.99997453\n",
      "  0.99997453 0.99997449]\n",
      " [0.9979721  0.99997409 0.99997453 0.99997452 0.99997452 0.99997449\n",
      "  0.99997452 0.99997453]\n",
      " [0.9979721  0.999974   0.99997453 0.99997453 0.99997453 0.99997452\n",
      "  0.99997453 0.99997453]]\n",
      "[[ 0.88076516  0.99336861  0.99881441  0.99966633  0.9997698   0.99969836\n",
      "   0.99983065  0.99949939]\n",
      " [ 0.88076516  0.97428855  0.99772081  0.99971629  0.99977544  0.99970564\n",
      "   0.99982903  0.99988845]\n",
      " [ 0.88076516  0.97428855  0.99772081  0.99920182  0.99948192  0.99969068\n",
      "   0.99985163  0.99953446]\n",
      " [ 0.88076516  0.95013817  0.98909643  0.99841151  0.99936527  0.9996957\n",
      "   0.99985478  0.99989455]\n",
      " [ 0.88076516  0.95013817  0.9652431   0.99120962  0.98892816  0.98843284\n",
      "   0.99777194  0.99805804]\n",
      " [ 0.88076516  0.97428855  0.99317804  0.99902195  0.99983581  0.99980886\n",
      "   0.99985599  0.99980546]\n",
      " [ 0.88076516  0.97428855  0.94413014  0.79897833  0.44491282  0.28817812\n",
      "   0.84844305  0.69283251]\n",
      " [ 0.88076516  0.95013817  0.69373539 -0.54336424 -0.77292671  0.47098384\n",
      "   0.9966119   0.99965951]\n",
      " [ 0.88076516  0.99336861  0.99301409  0.9924802   0.99672626  0.97469557\n",
      "   0.99805216  0.99820171]\n",
      " [ 0.88076516  0.99336861  0.99954409  0.99970418  0.99926251  0.99903697\n",
      "   0.99853847  0.99927406]]\n",
      "[[ 0.85328721  0.98321028  0.98961631  0.98745964  0.99777327  0.99685816\n",
      "   0.99816202  0.9992878 ]\n",
      " [ 0.85328721  0.89068232  0.71424127  0.49529674 -0.45130749 -0.6765511\n",
      "  -0.64741838  0.58540702]\n",
      " [ 0.85328721  0.98321028  0.98961631  0.94824279  0.95106009  0.93845143\n",
      "   0.9885739   0.99711356]\n",
      " [ 0.85328721  0.98321028  0.96343367  0.16112031  0.43325758  0.75913627\n",
      "   0.37884025 -0.03673002]\n",
      " [ 0.85328721  0.46525982 -0.3033153  -0.65918803 -0.96300017 -0.93627505\n",
      "   0.92929438  0.99951895]\n",
      " [ 0.85328721  0.89068232  0.71424127  0.49529674 -0.45130749 -0.6765511\n",
      "   0.60846565  0.97376161]\n",
      " [ 0.85328721  0.9362613   0.98455194  0.99694231  0.99933586  0.99967391\n",
      "   0.99972807  0.99973412]\n",
      " [ 0.85328721  0.46525982  0.56283208  0.75296737 -0.44500445 -0.79890269\n",
      "  -0.67279664  0.74138994]\n",
      " [ 0.85328721  0.46525982  0.56283208  0.83813813  0.95171468  0.94517036\n",
      "   0.65739727  0.94168764]\n",
      " [ 0.85328721  0.46525982  0.56283208  0.9538581   0.98929169  0.997154\n",
      "   0.98893897  0.980815  ]]\n",
      "[[ 0.85994066  0.915663    0.9092606   0.92659308  0.9715935   0.99812911\n",
      "   0.99826518  0.99904373]\n",
      " [ 0.85994066  0.59000236  0.09979083 -0.73305149 -0.77568103  0.10163486\n",
      "   0.93104595  0.98093044]\n",
      " [ 0.85994066  0.98518126  0.99244684  0.99243674  0.99761531  0.99583598\n",
      "   0.99394502  0.96480832]\n",
      " [ 0.85994066  0.59000236  0.74841938  0.98392912  0.98461433  0.99154292\n",
      "   0.98577455  0.95156162]\n",
      " [ 0.85994066  0.915663    0.9092606   0.92659308  0.93139474  0.99493156\n",
      "   0.99889272  0.99896979]\n",
      " [ 0.85994066  0.915663    0.85242338  0.26265504  0.7210096   0.89338731\n",
      "   0.75079237  0.95136625]\n",
      " [ 0.85994066  0.94939124  0.97328127  0.89131725  0.73907341  0.95428382\n",
      "   0.95191747  0.96689376]\n",
      " [ 0.85994066  0.915663    0.85242338  0.26265504  0.22418213  0.86843376\n",
      "   0.93530424  0.96435918]\n",
      " [ 0.85994066  0.915663    0.46441708 -0.01652834  0.15957653 -0.16213501\n",
      "   0.86352864  0.98781534]\n",
      " [ 0.85994066  0.915663    0.46441708  0.16301137  0.71154587  0.96169928\n",
      "   0.99264245  0.97911138]]\n",
      "[[ 0.85238765  0.56238229  0.26269149  0.65481042  0.95850726  0.99731276\n",
      "   0.99871626  0.99634644]\n",
      " [ 0.85238765  0.90390582  0.83348125  0.93639896  0.99364489  0.98713909\n",
      "   0.95065877  0.95447265]\n",
      " [ 0.85238765  0.90390582  0.95974428  0.99480489  0.9965787   0.9965143\n",
      "   0.99354458  0.9917133 ]\n",
      " [ 0.85238765  0.93956049  0.79877098  0.79029237  0.98643765  0.95819098\n",
      "   0.97622578  0.9910309 ]\n",
      " [ 0.85238765  0.56238229  0.72198006  0.98125515  0.93712377  0.8646067\n",
      "   0.86286259  0.57145367]\n",
      " [ 0.85238765  0.90390582  0.95974428  0.98762885  0.98951759  0.99604342\n",
      "   0.99802978  0.99825492]\n",
      " [ 0.85238765  0.93956049  0.96381381  0.98619495  0.99792874  0.99901468\n",
      "   0.99863035  0.99697505]\n",
      " [ 0.85238765  0.90390582  0.43069076 -0.72394458 -0.09691489  0.89221774\n",
      "   0.91263335  0.89222905]\n",
      " [ 0.85238765  0.90390582  0.89301968  0.70231145  0.83212375  0.97277646\n",
      "   0.95588397  0.99340118]\n",
      " [ 0.85238765  0.98150873  0.96789529  0.40280908  0.43206902  0.7832709\n",
      "   0.97161782  0.99068063]]\n",
      "[[ 0.83531999  0.85815072  0.90851571  0.92826897  0.93297408  0.99320893\n",
      "   0.99159686  0.98019029]\n",
      " [ 0.83531999  0.43777428 -0.78673096 -0.98860018 -0.35213091  0.99601119\n",
      "   0.99538806  0.96914828]\n",
      " [ 0.83531999  0.96969868  0.92768231  0.49806399  0.84516909  0.92210894\n",
      "   0.80648302  0.68299461]\n",
      " [ 0.83531999  0.43777428 -0.20746739  0.51765373  0.92315971  0.98064015\n",
      "   0.9862091   0.97710716]\n",
      " [ 0.83531999  0.85815072  0.09258363 -0.89602793 -0.50527815  0.98125176\n",
      "   0.97706496  0.73393089]\n",
      " [ 0.83531999  0.96969868  0.92768231  0.49806399  0.64024809  0.91298403\n",
      "   0.991784    0.98892922]\n",
      " [ 0.83531999  0.43777428  0.53465281  0.26790313 -0.27881657 -0.48482933\n",
      "  -0.58932247  0.97732471]\n",
      " [ 0.83531999  0.85815072  0.67796468  0.55334178  0.74125392  0.84469343\n",
      "   0.38832842  0.72333469]\n",
      " [ 0.83531999  0.85815072  0.67796468  0.55334178  0.74125392  0.96037891\n",
      "   0.93843689  0.66658163]\n",
      " [ 0.83531999  0.43777428  0.53465281  0.94785966  0.95509868  0.98821501\n",
      "   0.99200788  0.99161152]]\n",
      "[[ 0.82924353  0.84415329  0.06322595 -0.42461302  0.17721445  0.91246106\n",
      "   0.96431541  0.87471704]\n",
      " [ 0.82924353  0.89201611  0.88633504  0.82396578  0.86372048  0.89994459\n",
      "   0.76927969 -0.14414485]\n",
      " [ 0.82924353  0.96314109  0.9077189   0.53132129  0.72333603  0.63689548\n",
      "   0.86719279  0.80887168]\n",
      " [ 0.82924353  0.96314109  0.9077189   0.40319466  0.79256452  0.83672446\n",
      "   0.82496576  0.69564604]\n",
      " [ 0.82924353  0.89201611  0.84096838  0.63997865  0.21280587  0.04801798\n",
      "   0.80953571  0.87190548]\n",
      " [ 0.82924353  0.96314109  0.97722083  0.94262691  0.94886851  0.88305164\n",
      "   0.85598111  0.81484441]\n",
      " [ 0.82924353  0.89201611  0.53290049  0.44660486  0.93139003  0.9723327\n",
      "   0.98762569  0.9873668 ]\n",
      " [ 0.82924353  0.42460003 -0.77467503 -0.92577995 -0.8538098  -0.58023504\n",
      "   0.84707995  0.99193692]\n",
      " [ 0.82924353  0.96314109  0.9077189   0.53132129  0.72333603  0.63689548\n",
      "   0.96108515  0.98561373]\n",
      " [ 0.82924353  0.42460003 -0.05682429  0.69169999  0.48638075  0.09829482\n",
      "   0.83072895  0.95932599]]\n",
      "[[ 0.82524344  0.8380638   0.88568502  0.73977534 -0.39683026  0.38714788\n",
      "   0.93654573  0.86115915]\n",
      " [ 0.82524344  0.42366699 -0.75791723 -0.908479    0.15630009  0.99488005\n",
      "   0.9802051   0.6382986 ]\n",
      " [ 0.82524344  0.96015619  0.9887029   0.99135455  0.98016179  0.58967993\n",
      "   0.34453739  0.38832373]\n",
      " [ 0.82524344  0.8380638   0.09017388  0.2410063   0.25951561 -0.00368238\n",
      "   0.83261192  0.97816314]\n",
      " [ 0.82524344  0.88811157  0.88688779  0.76937611  0.61399771  0.86015855\n",
      "   0.93244925  0.79633951]\n",
      " [ 0.82524344  0.96015619  0.9887029   0.98395044  0.98370293  0.98791624\n",
      "   0.98804496  0.95612114]\n",
      " [ 0.82524344  0.8380638   0.88568502  0.73977534  0.43518053  0.79150483\n",
      "   0.88624484  0.98810801]\n",
      " [ 0.82524344  0.8380638   0.88568502  0.9687855   0.88764913  0.72710121\n",
      "   0.94058237  0.97663622]\n",
      " [ 0.82524344  0.88811157  0.54747856 -0.14552695  0.16149323  0.79488912\n",
      "   0.94700562  0.90415753]\n",
      " [ 0.82524344  0.96015619  0.97542125  0.78599981 -0.03563808  0.3594444\n",
      "   0.9697998   0.97579605]]\n",
      "[[ 0.81655356  0.38555615 -0.76286721 -0.85786518  0.84262385  0.99880009\n",
      "   0.99207832  0.90006907]\n",
      " [ 0.81655356  0.86632483  0.4736445  -0.20940478 -0.03323688  0.86687754\n",
      "   0.98180203  0.94433073]\n",
      " [ 0.81655356  0.81329353  0.59563798  0.34545553  0.47136847  0.93383205\n",
      "   0.8921317   0.6046826 ]\n",
      " [ 0.81655356  0.81329353  0.69432438  0.70478599  0.66585582  0.61482846\n",
      "   0.86975812  0.92805627]\n",
      " [ 0.81655356  0.81329353  0.85565278  0.67196479  0.66279472  0.93730741\n",
      "   0.8110785   0.60669516]\n",
      " [ 0.81655356  0.38555615 -0.22056022  0.55480691  0.92513792  0.70005863\n",
      "   0.68902446  0.8819627 ]\n",
      " [ 0.81655356  0.86632483  0.93114178  0.96509696  0.97076661  0.88842158\n",
      "   0.60154782  0.67146436]\n",
      " [ 0.81655356  0.9494437   0.98283655  0.97265514  0.91371836  0.66629701\n",
      "   0.80949423  0.08313268]\n",
      " [ 0.81655356  0.9494437   0.87412619  0.6860499   0.87836657  0.8465518\n",
      "   0.89002448  0.77213729]\n",
      " [ 0.81655356  0.38555615  0.45686279  0.29444027 -0.77438553 -0.49618438\n",
      "   0.93012875  0.99507417]]\n",
      "[[ 0.8059108   0.33284294 -0.3118783  -0.81207946 -0.95750753 -0.12528039\n",
      "   0.96179457  0.96064645]\n",
      " [ 0.8059108   0.33284294 -0.17333872 -0.6552643  -0.04149749  0.97589071\n",
      "   0.90096695  0.85030952]\n",
      " [ 0.8059108   0.77584927  0.48030183 -0.5055426  -0.0355206   0.96224114\n",
      "   0.78365649  0.65692593]\n",
      " [ 0.8059108   0.77584927  0.59258544  0.76506642  0.75265921  0.793069\n",
      "   0.91254074  0.83067175]\n",
      " [ 0.8059108   0.77584927 -0.1023811  -0.00871055  0.90396589  0.91065558\n",
      "   0.71640764 -0.02339439]\n",
      " [ 0.8059108   0.93363207  0.97068613  0.8382396  -0.01906101 -0.78462982\n",
      "  -0.72976398  0.85421376]\n",
      " [ 0.8059108   0.83668398  0.72338033  0.67661372  0.90678543  0.93192751\n",
      "   0.95602012  0.76097163]\n",
      " [ 0.8059108   0.83668398  0.78878082  0.65090712  0.68641923  0.23371543\n",
      "   0.12570578  0.15648093]\n",
      " [ 0.8059108   0.33284294 -0.78663593 -0.97886164 -0.93737184  0.88554265\n",
      "   0.99652712  0.82176462]\n",
      " [ 0.8059108   0.93363207  0.91973557  0.56293434  0.49342693  0.27653824\n",
      "   0.4176947   0.51870258]]\n",
      "Epoch: 0 Train Error: 0.04127746791740741\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "data = data_generator(1000)\n",
    "rnn_object = Rnn(input_dim=2)\n",
    "rnn_object.train(train_data=data, batch_generator=input_feed, batch_size=10, epochs=1)\n",
    "# print(int2binary[num_range/2-1 + num_range/2-1 ] )\n",
    "# print(generate_row(num_range/2-1,num_range/2-1)[0:_DIGITS])\n",
    "# row = generate_row(np.random.randint(int(num_range/2)),np.random.randint(int(num_range/2)))\n",
    "# print(row[0,_DIGITS*2:])\n",
    "# print(rnn_object._predict(row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
